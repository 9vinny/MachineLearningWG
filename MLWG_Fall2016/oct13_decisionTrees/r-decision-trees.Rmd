---
title: "Decision trees for machine learning"
output: html_notebook
---

Topics

* rpart
* Caret
* SuperLearner
* h2o.ai
* book

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Use the latest RStudio preview release to run within RStudio.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
# Load iris dataset.
data(iris)

# Review data structure.
str(iris)

# Review species distribution.
table(iris$Species, useNA = "ifany")
```

```{r}
# install rpart first if you don't already have it.
# rpart = recursive partitioning and regression trees (aka decision trees)
library(rpart)

# Review package help and vignette if available.
# HINT: vignette covers all of this in much better detail.
help(package = "rpart")

# Review main decision tree function.
?rpart

# Review the configuration options for trees.
?rpart.control

# Fit a classification decision tree to predict Species using all other variables.
# We don't need to specify method="class" because Species is a factor variable.
tree_model = rpart(Species ~ ., data = iris,
            control = rpart.control(xval = 10, minbucket = 5, cp = 0))

# Display the decision tree in text form.
tree_model


# Plot tree graphically.
plot(tree_model, compress = T)
# We have to add the plot text manually for some reason.
text(tree_model, use.n = T)
```

Wow, this is one of the worst plots I've ever seen! Hard to get much worse than that.

Let's tree a better decision tree plotting package.

```{r}
# Install from CRAN if you don't already have this:
library(rpart.plot)

rpart.plot(tree_model)

# What other settings can we modify?
?rpart.plot

# Review the vignette if interested.
help(package = "rpart.plot")

# fancyRpartPlot() in the rattle package is also good.
```

We can dig into the details of the tree a bit more.

```{r}
# Review accuracy for different complexity parameters.
# When nsplits = 0 we have 0 nodes and are just guessing the most common class.
# When nsplits is large we have 1  + # splits nodes and each node is its own prediction.
printcp(tree_model)

# Get all the details on the tree.
summary(tree_model)
```

We did not create a separate holdout or test set, so let's predict back on the original data.

```{r}
predictions = predict(tree_model, iris)
summary(predictions)

# How do the predictions look compared to the outcome data?
data.frame(iris$Species, predictions)

# This is an optimistic view because the model was built on this same data.
# With a random holdout set we would get a more realistic view of accuracy.

```

## Regression

Quick regression example.
```{r}
# This data is in the rpart package.
data(car90)

# Review structure of dataset.
str(car90)

# Predict price using all other fields.
reg_tree = rpart(Price ~ ., data = car90)

# How'd it go?
reg_tree

# Visualize this bad boy.
rpart.plot(reg_tree)
```

# Caret

```{r}
library(caret)

# Nice and simple - using default settings for everything.
# caret tries 3 complexity parameters by default, but tuneLength would customizex that.
model = train(Species ~ ., data = iris, method = "rpart")

# We see again that cp= 0 gives us the best accuracy.
model

# Use the handle built-in caret plotting.
plot(model)
```

# SuperLearner

SuperLearner unfortunately cannot do multiple-class classification (yet) so let's convert to a binary classification problem.

```{r}

# Review 
table(iris$Species)

# Copy into a new dataframe.
data = iris

# Convert Species to a binary indicator for setosa.
data$Species = 1*(data$Species == "versicolor")

# Confirm distribution of modified outcome variable.
table(data$Species, iris$Species, useNA = "ifany")

library(SuperLearner)

sl = SuperLearner(X = data[, -5], Y = data$Species, SL.library = c("SL.mean", "SL.rpart"),
                  family = binomial())
sl

# Review the raw rpart object.
sl$fitLibrary$SL.rpart_All$object

# Use our nice plotting library.
rpart.plot::rpart.plot(sl$fitLibrary$SL.rpart_All$object)

```

# h2o.ai

```{r}
library(h2o)

# Start h2o backend.
h2o.init()

# Load iris data into h2o.
iris_h2o = h2o.uploadFile(path = system.file("extdata", "iris_wheader.csv", package="h2o"), destination_frame = "iris_h2o")

# Confirm it loaded correctly.
summary(iris_h2o)

# Specify x and y by the column indices. Set ntree to 1, and mtries to # of covariates.
iris_tree = h2o.randomForest(y = 5, x = 1:4, training_frame = iris_h2o, ntrees = 1,
                             mtries = 4)

# Review results.
iris_tree

summary(iris_tree)

# Review variable importance.
h2o.varimp(iris_tree)

# Plot variable importance - nice.
h2o.varimp_plot(iris_tree)

# Shutdown h2o backend.
h2o.shutdown(prompt = F)
```


# Decision tree references

This book has nearly everything you would want to know about the theory of decision trees:

Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.

The book has 32,000 citations according to Google Scholar. Not too shabby! Breiman and Stone were both Berkeley professors, and Breiman invented Random Forest, bagging, and some of the SuperLearner theory. Friedman is at Stanford and invented many other machine learning algorithms, particularly gradient boosted machines GBM) and multivariate adaptive regression splines (MARS). Olshen is also at Stanford.
