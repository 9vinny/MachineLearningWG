{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: There and Back Again...and Again...and Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most difficult tasks in machine learning is feature engineering. Feature engineering is where you create new features from the ones you have through mathematical operations such as multiplication. For example, making $x^2$ from $x$. This is done because many features end up being correlated and have nonlinear relationships with the output. Neural networks are useful because they automate feature engineering in a simple and straightforward way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\">\n",
    "Taken from the MIT CS231n <a href=\"http://cs231n.github.io/neural-networks-1/\">course page</a>. <a href=\"https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\">LICENSE</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each circle in the diagram is called a neuron or node. Each feature in your input data gets a node. Each class in your output gets a node. In the above picture, there are two outputs, so it's a binary classification problem. To get from the input to the output you go through any number of hidden layers with any number of nodes; the numbers are hyperparameters. You can see that all the input nodes contribute to each hidden layer node somehow. The weights for how much each input contributes are the parameters to be optimized. It's called a hidden layer because the user doesn't get to see the parameters without purposefully looking for them. Thus, feature engineering is automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets are mainly used for classification, so that's the problem I'm going to focus on. I'll explain how it can be modified later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication: A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to figure out how to code an algorithm, it is helpful to keep track of all the dimensions of the matrices and vectors you're working with. Remember what happens with the dimensions:\n",
    "$$ [n \\times m][m \\times p] = [n \\times p] $$\n",
    "The inner two dimensions have to be the same; the operation is not defined otherwise. The product matrix then takes the outer dimensions. For example, take these two vectors (1D matrices):\n",
    "$$ \\mathbf{A} =  \\begin{bmatrix}\n",
    "                    a_1 \\newline a_2 \\newline a_3\n",
    "                 \\end{bmatrix},\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "                b_1 \\newline b_2 \\newline b_3\n",
    "             \\end{bmatrix} $$\n",
    "They are both $[3 \\times 1]$ vectors. They can't be multiplied together as is, but if you take the transpose of one, they can be. This gives us two possibilities:\n",
    "$$ \\mathbf{A^TB} = a_1b_1 + a_2b_2 + a_3b_3 \\\\\n",
    "   \\mathbf{AB^T} = \\begin{bmatrix}\n",
    "                        a_1b_1 & a_1b_2 & a_1b_3 \\\\\n",
    "                        a_2b_1 & a_2b_2 & a_2b_3 \\\\\n",
    "                        a_3b_1 & a_3b_2 & a_3b_3\n",
    "                   \\end{bmatrix} \\\\ $$\n",
    "Notice that the answers are quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the above looks like as a summation:\n",
    "$$ (\\mathbf{AB})_{ij} = \\sum_{k=1}^m A_{ik}B_{kj} $$\n",
    "where $i$ goes from 1 to $n$ and $j$ goes from 1 to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression: A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review the math behind binary logistic regression. I'll be using the notation from <a href=\"https://www.coursera.org/learn/machine-learning/home/welcome\">Machine Learning by Andrew Ng</a> on Coursera, except for two changes. One, I'm going to treat the parameter and feature vectors as row vectors, while Andrew Ng treats them as column vectors. Two, I'm going to keep the bias terms as a separate vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a binary classification problem with classes 0 and 1. You have a hypothesis about your data, $ h_{\\theta}(x) $, which is the probability that the item with feature $x$ is in class 1. The probability of class 0 is $1 - h_{\\theta}(x) $. What does this hypothesis look like? We need a function that can take any real number and squish it into the interval [0,1]. This is called the activation function. One such function is the sigmoid, or logistic, function:\n",
    "$$ g(z) = \\frac {1}{1+e^{-z}} $$\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/sigmoid.jpeg\">\n",
    "Taken from the MIT CS231n <a href=\"http://cs231n.github.io/neural-networks-1/\">course page</a>. <a href=\"https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\">LICENSE</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $ h_{\\theta}(x) = g(z) \\geq 0.5$, we predict class 1. When $ h_{\\theta}(x) = g(z) < 0.5$, we predict class 0. Well, what is $z$? If we have $n$ features:\n",
    "$$ z = b + \\theta_1x_1 + ... + \\theta_nx_n = b + \\begin{bmatrix}x_1 &  x_2 &  ...  \\hspace{2em}  x_n\\end{bmatrix}\\begin{bmatrix} \\theta_1 \\newline \\theta_2 \\newline \\vdots \\newline \\theta_n\\end{bmatrix}= b + \\boldsymbol{x \\theta}^T $$\n",
    "where $\\theta$ and $x$ are column vectors. $\\theta$ are the parameters to optimize. $b$ is called the bias term and it acts much like an intercept term in linear regression. It allows you to shift the activation function to get a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for this problem is:\n",
    "$$ J(\\boldsymbol{\\theta}) = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta (\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log (1 - h_\\theta(\\mathbf{x}^{(i)}))] $$\n",
    "where $m$ is the number of observations. Basically, if you think about a DataFrame, $i$ is the row and $m$ is the total number of rows. Then $\\mathbf{x}$ is $[m \\times n]$ and $\\mathbf{y}$ is $[m \\times 1]$. In vectorized notation, this is:\n",
    "$$ J(\\boldsymbol{\\theta}) = - \\frac{1}{m} [\\mathbf{y}^T \\log (g(b+\\boldsymbol{x \\theta}^T)) + (\\mathbf{1 - y}^T) \\log (g(1 - b+\\boldsymbol{x \\theta}^T))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a regularization term:\n",
    "$$ J(\\boldsymbol{\\theta}) = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta (\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log (1 - h_\\theta(\\mathbf{x}^{(i)}))] + \\frac{\\lambda}{2m} \\sum^{n}_{j=1} \\theta_j$$  \n",
    "\n",
    "And in vector notation:\n",
    "$$ J(\\boldsymbol{\\theta}) = - \\frac{1}{m} [\\mathbf{y}^T \\log (g(b+\\boldsymbol{x \\theta}^T)) + (\\mathbf{1 - y}^T) \\log (g(1 - b+\\boldsymbol{x \\theta}^T))] + \\frac{\\lambda}{2m}\\boldsymbol\\theta\\boldsymbol\\theta^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can minimize the cost function with the solver of your choice. Often times, the solver asks for a cost function and a derivative, so here's the derivative for all $\\boldsymbol{\\theta}$'s:\n",
    "$$ \\frac{\\partial}{\\partial\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m  (h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) \\mathbf{x}_j^{(i)} \\\\\n",
    "= \\frac{1}{m} \\mathbf{x}^T(g(b+\\boldsymbol{x \\theta}^T) - \\mathbf{y})\n",
    "$$\n",
    "And here's the derivative for the bias term: \n",
    "$$ \\frac{\\partial}{\\partial b} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m  (h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) \\mathbf{x}_j^{(i)} \\\\\n",
    "= \\frac{1}{m} (g(b+\\boldsymbol{x \\theta}^T) - \\mathbf{y})\n",
    "$$\n",
    "To add regularization to this, add $ \\frac {\\lambda}{m}\\theta_j $ to the jth $\\boldsymbol{\\theta}$ derivative. Note that the bias term doesn't have regularization, so its derivative doesn't add regularization either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest solver to use is gradient descent and it's exactly what it sounds like. If you imagine a graph of the cost function vs the parameters, the minimum is the bottom of the valley in the graph. You have to descend the slope, or gradient, to get there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5b/Gradient_descent_method.png\">\n",
    "By Роман Сузи (Сгенерирован программой, повернут вручную) [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the algorithm, the estimates for $\\boldsymbol\\theta$ are updated according to their gradients, the cost function is recalculated, and this is repeated until convergence. Here are the equations for updating $\\boldsymbol\\theta$:\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol\\theta) $$\n",
    "$\\alpha$ is called the learning rate, and it's a hyperparameter that determines how fast the algorithm converges. If it is too large, the algorithm could keep missing the minimum and never converge. If it's too small, the algorithm could take a long time to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakfast: Neural Nets from scratch\n",
    "Neural nets just apply logistic regression at every neuron past the input layer. To keep things simple, I'm going to stick with a binary classification problem modified from <a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\">this</a> blog post. So, let's use scikit learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">```make_moons()```</a> function to get some binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [9,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a dataset\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 200 observations and 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that numpy doesn't distinguish between row and column vectors by default. Everything is just a 1D array. This could get confusing when delving into the math, so let's turn it into a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y, (200,1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's what it might look like in a DataFrame\n",
    "data = np.column_stack((X, y))\n",
    "data = pd.DataFrame(data, columns=[\"Height\", \"Daily Caloric Intake\", \"Hobbit?\"])\n",
    "data[\"Hobbit?\"] = data[\"Hobbit?\"].astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot it\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, it's easy to see that the division between hobbits and not hobbits is not linear, so logistic regression by itself won't work without some feature engineering. That's why we throw it into a neural net and let it do the engineering for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "First, we start with our inputs and calculate forward to the output neurons using initial guesses for all $\\boldsymbol\\theta$'s. In our example, we'll have one hidden layer with 5 neurons. \n",
    "<img src=\"nn-from-scratch-3-layer-network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity's sake, the bias terms are not usually included in these types of diagrams. The $\\mathbf a$'s are the activation units. $\\mathbf a^{(1)}$ is just $\\mathbf X$, plus a column of one's for the bias variable. Input 1 is \"Height\" and Input 2 is \"Daily Caloric Intake.\" In a single logistic regression, $\\boldsymbol\\theta^{(1)}$ would be $[1 \\times 2]$. But, we're doing 5 logistic regressions, one for each neuron in the hidden layer, so $\\boldsymbol\\theta^{(1)}$ is $[5 \\times 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hl_nodes = 5\n",
    "np.random.seed(37)\n",
    "theta1 = np.random.randn(n_hl_nodes, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When initializing $\\boldsymbol\\theta$, it is important to break symmetry. If $\\boldsymbol\\theta$ is symmetric, then some of the nodes in the hidden layer will turn out to be the same and the algorithm will not be as efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias vector is going to be $[1 \\times 5]$, one bias term per hidden node/logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = np.random.randn(1, n_hl_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, $\\mathbf a^{(2)}$ is just $g(b+\\boldsymbol{x \\theta}^T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our activation function\n",
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.shape #same as X, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z1 = np.dot(a1, theta1.T) + b1 #broadcasting! :O\n",
    "a2 = sigmoid(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a2.shape #shape is (m, n_hl_nodes), check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we've turned our 2 feature problem into a 5 feature problem now. The next step is to use $\\mathbf a^{(2)}$ like it's our new $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "np.random.seed(42)\n",
    "theta2 = np.random.randn(n_classes, n_hl_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b2 = np.random.randn(1, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z2 = np.dot(a2, theta2.T) + b2\n",
    "a3 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.shape #shape is (m, n_classes), check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf a^{(3)}$ is $ h_{\\theta}(x)$. You might also realize it's not the same size as $y$. What we need to do is make a $y_{big}$ matrix that marks a 1 in the row under the column that corresponds to the class. Say our first observation is a hobbit. The first row gets a 1 in the hobbit column. The second observation is not a hobbit, so the second row gets a 1 in the not hobbit column. Since we're doing a binary problem, this works out to $[y, 1-y]$, but it can be generalized to any number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_big = np.column_stack((y, 1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_big.shape #same shape as a3, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_big[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the cost. We need to calculate it per class, which means matrix math won't quite do the trick. We have to loop over the columns. First without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J = 0\n",
    "for i in range(n_classes):\n",
    "    J += -1 / m * (np.dot(y_big[:,i].T,np.log(a3[:,i])) + np.dot((1 - y_big[:,i]).T, np.log(1 - a3[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_lambda = 0.01 # remember that lambda is a reserved word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J += reg_lambda / (2 * m) * (np.dot(theta1.flatten().T, theta1.flatten()) +\n",
    "                             np.dot(theta2.flatten().T, theta2.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "We've got our first predictions. Now we need to compare them to our labels and use this to update our parameters. This isn't quite as simple as just applying your solver to the layers. We'll go through these step by step. The first thing to do is figure out how to calculate our residuals, $\\boldsymbol\\delta$. $\\boldsymbol\\delta^{(3)}$ is easy:\n",
    "$$ \\boldsymbol\\delta^{(3)} = \\mathbf a^{(3)} - \\mathbf y_{big} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3 = a3 - y_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.shape #same shape as a3 and y_big, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining layers, except for the input layer, the residual is calculated as follows:\n",
    "$$ \\boldsymbol\\delta^{(l)} = \\boldsymbol\\delta^{(l+1)} \\boldsymbol\\theta^{(l)} .* g'(\\mathbf z^{(l)}) $$\n",
    "where $l$ is the layer number and I'm using $.*$ to emphasize that this is element-wise multiplication, not matrix multiplication. Turns out that $g'(\\mathbf z^{(l)})$ is just $\\mathbf a^{(l)} .* (1 - \\mathbf a^{(l)})$, so:\n",
    "$$ \\boldsymbol\\delta^{(l)} = \\boldsymbol\\delta^{(l+1)} \\boldsymbol\\theta^{(l)} .* \\mathbf a^{(l)} .* (1 - \\mathbf a^{(l)}) $$\n",
    "\n",
    "There is no residual for the input layer ($l=1$), so we only have $\\boldsymbol\\delta^{(2)}$ left to calculate:\n",
    "$$ \\boldsymbol\\delta^{(2)} = \\boldsymbol\\delta^{(3)} \\boldsymbol\\theta^{(2)} .* \\mathbf a^{(2)} .* (1 - \\mathbf a^{(2)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = np.dot(d3, theta2) * a2 * (1 - a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.shape #same shape as a2, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to turn this into an overal gradient that we can feed to a solver. First, without regularization, the equation for $\\boldsymbol\\theta$ is:\n",
    "$$\\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}} = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)}$$\n",
    "\n",
    "Vectorized:\n",
    "$$ \\dfrac{\\partial J(\\boldsymbol\\Theta)}{\\partial \\boldsymbol\\Theta^{(l)}} = \\frac{1}{m} \\sum^l \\mathbf a^{(l)} \\boldsymbol\\delta^{(l+1)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D2 = np.dot(d3.T, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2.shape #same shape as theta2, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1 = np.dot(d2.T, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1.shape #same shape as theta1, check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $b^{(l)}$, it's just $\\mathbf{I}^{(l)T}\\boldsymbol\\delta^{(l+1)}$, where $\\mathbf{I}^{(l)}$ is an identity vector with the same number of rows as $\\mathbf a^{(l)}$. This works out to summing the columns of $\\mathbf a^{(l)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B2 = np.sum(d3, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B1 = np.sum(d2, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization term is easy. Just add this:\n",
    "$$ \\frac {\\lambda}{2 * m} \\boldsymbol\\Theta^{(l)} $$\n",
    "to this:\n",
    "$$ \\dfrac{\\partial J(\\boldsymbol\\Theta)}{\\partial \\boldsymbol\\Theta^{(l)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1 = D1 + theta1 * reg_lambda\n",
    "D2 = D2 + theta2 * reg_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the bias terms don't get regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing\n",
    "Now that we have our cost function and our derivative, we can use gradient descent to solve it. Here's what the first update looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "theta1 += -alpha * D1\n",
    "theta2 += -alpha * D2\n",
    "b1 += -alpha * B1\n",
    "b2 += -alpha * B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy. Now we just keep going until the cost stops changing by more than our tolerance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's collect the forward propagation steps\n",
    "def forward_propagation(a1, theta1, b1, theta2, b2, y_big):\n",
    "    z1 = np.dot(a1, theta1.T) + b1\n",
    "    a2 = sigmoid(z1)\n",
    "    z2 = np.dot(a2, theta2.T) + b2\n",
    "    a3 = sigmoid(z2)\n",
    "    J = 0\n",
    "    for i in range(n_classes):\n",
    "        J += -1 / m * (np.dot(y_big[:,i].T,np.log(a3[:,i])) + np.dot((1 - y_big[:,i]).T, np.log(1 - a3[:,i])))\n",
    "        J += reg_lambda / (2 * m) * (np.dot(theta1.flatten().T, theta1.flatten()) + \n",
    "                                     np.dot(theta2.flatten().T, theta2.flatten()))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's collect the back propagation steps\n",
    "def back_propagation(a3, y_big, theta2, a2, a1, theta1, reg_lambda):\n",
    "    d3 = a3 - y_big\n",
    "    d2 = np.dot(d3, theta2) * a2 * (1 - a2)\n",
    "    D1 = np.dot(d2.T, a1)\n",
    "    D2 = np.dot(d3.T, a2)\n",
    "    B2 = np.sum(d3, axis=0, keepdims=True)\n",
    "    B1 = np.sum(d2, axis=0, keepdims=True)\n",
    "    D1 = D1 + theta1 * reg_lambda\n",
    "    D2 = D2 + theta2 * reg_lambda\n",
    "    \n",
    "    return D1, D2, B1, B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tolerance = 0.0001\n",
    "delta = 1\n",
    "old_J = J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#again and again and again\n",
    "while delta > tolerance:\n",
    "    #there\n",
    "    J = forward_propagation(a1, theta1, b1, theta2, b2, y_big)\n",
    "    \n",
    "    #and back\n",
    "    D1, D2, B1, B2 = back_propagation(a3, y_big, theta2, a2, a1, theta1, reg_lambda)\n",
    "    \n",
    "    theta1 += -alpha * D1\n",
    "    theta2 += -alpha * D2\n",
    "    b1 += -alpha * B1\n",
    "    b2 += -alpha * B2\n",
    "    \n",
    "    delta = old_J - J\n",
    "    old_J = J\n",
    "    print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the cost function isn't quite minimized. This is the down side of gradient descent; it tends to overshoot. We could spend a bit more time trying to debug this (which will probably involve rearranging the loop)...or you get the gist of it and we can move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification\n",
    "We made several choices in our implemetation: the activation function, the cost function, and the solver. Any of these can be changed to suit your purposes. For example, you can use the sum of squares as the cost function to use neural nets for regression. The blog post I referenced used a different function in the final layer than for the hidden layers. The catch is that you have to rederive all the propagation steps appropriately...or find someone who's done that already. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second breakfast: Prebaked neural nets\n",
    "### scikit learn\n",
    "There are quite a few Python modules that can do neural nets. For small problems, scikit learn is more than adequate if you want one stop ML shopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to do what we did from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=0.01, hidden_layer_sizes=(5,), #alpha is our lambda!\n",
    "                    activation='logistic', learning_rate_init=0.0005, \n",
    "                    random_state=0) #we used several seeds, so I'll use the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier likes 1D arrays\n",
    "y = y.reshape(m)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are parameters for the activation function and the solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more interesting example using the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits\">digits dataset</a>. It consists of pictures of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(return_X_y=True)\n",
    "digits = load_digits()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Let's change some parameters to try to get it to converge. Let's use the default solver (adam), the default activation function (relu), and two hidden layers with 10 nodes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(alpha=0.01, hidden_layer_sizes=(10, 10), # the numbers in the tuple are how many nodes in a layer\n",
    "                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "For anything even remotely expensive, I recommend Keras. It's a Python front end for building any neural network architecture you can dream up. There's also a wrapper for it in R called kerasR. On the backend, you can have Tensorflow, CNTK, or Theano. The default backend is Tensorflow because Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same NN that we did above for the digits dataset, just for comparison. In deep learning, the type of neural network we've been doing (multi-layer perceptron) is always the final step. It is called the fully connected layer, or the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a seed\n",
    "RandomUniform(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "n_classes = 10\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential() #we're going to add all the layers one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the layers\n",
    "model.add(Dense(10, input_shape=X_train.shape[1:])) #1st hidden layer with 10 nodes\n",
    "model.add(Activation('relu')) #activation function\n",
    "model.add(Dense(10)) #2nd hidden layer with 10 nodes\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_classes)) #output layer\n",
    "model.add(Activation('softmax')) #turns out sklearn always uses softmax at the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun fact: The default configuration for Adam is actually the same for scikit learn and keras. Turns out they both just stole the default parameter values from the original paper. You can mess with the parameters in both, but I'll just use the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configure the optimizer\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the learning process\n",
    "model.compile(loss='categorical_crossentropy', #this is our logistic regression loss function\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=200,\n",
    "          epochs=200,\n",
    "          validation_data=(X_test, y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than scikit learn!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
