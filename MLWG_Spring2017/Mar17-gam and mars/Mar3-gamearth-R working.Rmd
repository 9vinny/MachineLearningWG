---
title: "Generalized additive models (GAMs) and Multivariate adaptive regression splines (MARS/EARTH) - rough draft"
author: "Evan Muzzall"
date: "3/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1. Package installation  
We will use the following packages for this example:
```{r}
if (F) {
  install.packages(c("akima", "caret", "devtools", "earth", "gam", "ggplot2", "mgcv", "mlbench", "plotmo")) # run lines 16 and 17 manually if needed
  devtools::install_github("ck37/ck37r")
}

library(akima)
library(caret)
library(ck37r)
library(devtools)
library(gam)
library(ggplot2)
library(mgcv)
library(mlbench)
library(plotmo)
library(earth)
```

##2. Goals
Using the "PimaIndiansDiabetes2" dataset, construct a generalized additive models (GAM), and multivariate additive regression models (MARS, aka EARTH) to predict blood pressure via the other variables as predictors. Missing data will be median-imputed and indicators will be created to document their missingness.  

##3. Preprocess the data
```{r}
# load the dataset
data(PimaIndiansDiabetes2)
?PimaIndiansDiabetes2
data <- PimaIndiansDiabetes2 # give the data a simpler name
str(data)
```

Check for missing data:
```{r}
# check for missing cases
sum(is.na(data)) 

# how much of the data is missing? 
sum(is.na(data)) / (nrow(data)*ncol(data)) # about 9% 
```

Recode the "diabetes" vector to numeric type:
```{r}
data$diabetes <- ifelse(data$diabetes=="pos", 1, 0)
```

Use Chris K's handy median impute function to impute missing values: 
```{r}
# impute and add missingness indicators
result = ck37r::impute_missing_values(data) 

# overwrite "data" with new imputed data frame
data <- result$data 
```

Double check that missing values have been imputed:
```{r}
# no more NA values
sum(is.na(data))

# check that missingness indicators have been added
str(data)
```

Split the data
```{r}
set.seed(1)
split <- createDataPartition(data$pressure, p=0.70, list=FALSE)
train <- data[split, ]
test <- data[-split, ]
```

##4. Generalized additive models (GAMs)
This semester, MLWG has explored linear, polynomial, and spline regression models using single predictors (March 3) as well as stepwise selection using multiple predictors (Feb 17). Last semester, we talked about improving linear regression models via penalized regression (LASSO and ridge) using multiple predictors (Nov 4).  

When considering multilple predictor variables, another extension of multiple linear regression can be used - generalized additive models.  

Generalized additive models (GAMs) are another extension of multiple linear regression. Not bound by linear relationships between predictor and response variable, GAMs can instead incorporate smoothed, nonlinear relationships. Each relationships is computed and summed ("additive"). Smoothed splines are not the only constructs used to build GAMs. Interestingly, GAMs can also be built from natural splines, local regression, polynomial regression, etc.  

However, for a smoothed spline GAM, least squares cannot be used to estimate the coefficients. This is handled by "backfitting", or updating the model as each predictor is approximated.  

See [Wood's book](https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R/Wood/p/book/9781584884743) for thorough walkthroughs of GAMs in R. 

As always, we also encourage [Introduction to Statistical Learning - Chapter 7](http://www-bcf.usc.edu/~gareth/ISL/) for a nice introductory overview and exercises.  

Fit the GAM: 
```{r}
gam1 <- gam(pressure ~ s(pregnant) + s(glucose) + s(triceps) + s(insulin) + s(mass) + s(pedigree) + s(age) + diabetes,
            # + miss_glucose + miss_pressure + miss_triceps + miss_insulin + miss_mass, 
            family="gaussian",
            data=train)

gam1
# view summary output
gam.check(gam1)

summary(gam1$fit)

names(gam1)

gam1$aic

# include MSE?
```

Play with some basic plotting features
```{r}
plot(gam1, se=T, # seWithMean=F,
         shade=T, col="black", shade.col="gray80", # responsible color scheme
         residuals=F,
         pages=1)
title("GAM - train data")
```

##5. Compare the GAM to other similar GAMs!
Our plots suggest that "pregnantn" is fairly linear. What if we compare `gam1` to two other GAMs - one that _excludes_ the predictor pregnant, and another that _assumes a linear relationship_ of pregnant?
```{r}
# model that excludes pregnant
gam2 <- gam(pressure ~ s(glucose) + s(triceps) + s(insulin) + s(mass) + s(pedigree) + s(age) + diabetes,
            family="gaussian",
            data=train)

# model that assumes linear insulin
gam3 <- gam(pressure ~ pregnant + s(glucose) + s(triceps) + insulin + s(mass) + s(pedigree) + s(age) + diabetes,
            family="gaussian",
            data=train)

anova(gam1, gam2, gam3, test="F") # small p-value suggests that a non-linear function for insulin might be preferable?
```

Test on new data? 
```{r}
gam1_test <- gam(pressure ~ s(pregnant) + s(glucose) + s(triceps) + s(insulin) + s(mass) + s(pedigree) + s(age) + diabetes,
                 family="gaussian",
                 data=test)

gam1_test$aic # 1821.676 means overfit? (gam1 aic = 4010.727)

plot(gam1_test, se=T, # seWithMean=F,
         shade=T, col="purple", shade.col="orange", # fancy color scheme
         residuals=F,
         pages=1)
title("GAM - test data")
```

What if we want to identify unhelpful predictors and remove them for better results?
```{r}
table(train$diabetes, I(train$pressure<115))

gam4 <- gam(pressure ~ s(pregnant) + s(glucose) + s(triceps) + s(insulin) + s(mass) + s(pedigree) + s(age) + diabetes,
            family="gaussian",
            data=train,
            subset=(diabetes !=0))

plot(gam4, se=TRUE, seWithMean=TRUE, 
         shade=TRUE, col="blue", shade.col="seagreen",# obnoxious color scheme
         residuals=FALSE,
         pages=1)
title("GAM - adjusted predictors")
```

##6. plotmo
The "plotmo" R package offers a great way to visualize regression splines in three dimensions:
```{r}
plotmo(gam1, all2=TRUE) # show simplfied seWithMean plots AND three dimensional splines for all variable relationships

# plot partial dependencies (takes a few minutes)
# plotmo(gam1, all2=TRUE, pmethod = "partdep") 

# faster version of pmethod="partdep"
plotmo(gam1, all2=TRUE, pmethod = "apartdep", 
       caption = "What have I gotten myself in to...") 

# let's play around with a few more parameters! 
plotmo(gam1, all2=TRUE, pt.col = "green3")
plotmo(gam1, all2=TRUE, pt.col = "green3", smooth.col = "red")
plotmo(gam1, all2=TRUE,  
       pt.col = "green3", 
       smooth.col = "red",
       grid.col="gray80")

# return just some of the plots! 
plotmo(gam1, all2=TRUE, degree1 = c(1,2), degree2=0, col="tomato") # show just the first two predictor plots

plotmo(gam1, all2=TRUE, degree1 = 0, degree2 = 1, # return just glucose v. pregnant perspective plot
       caption = "this is called a 'perspective plot'",
       persp.col="cyan")


```

See [Wood S. 2006. Generalized additive models: An introduction with R](https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1584884746) for expert explanations.  

["gam" R package](https://cran.r-project.org/web/packages/gam/index.html)

["mgcv" R package](https://cran.r-project.org/web/packages/mgcv/mgcv.pdf)

Also check out [Stephen Milborrow's excellent instructions on the "plotmo" R package](http://www.milbo.org/doc/plotmo-notes.pdf)

##7. Multivariate adaptive regression splines (MARS) and (earth)
Multivariate adaptive regression splines (MARS) are a technique developed by Jerome H. Friedman in 1991 and copyrighted by Salford Systems. Open source implementations are thusly referred to as "earth", 

earth = Enhanced Adaptive Regression Through Hinges   

These approaches use "surrogate features" (or, models of the models), usually versions of one or two predictors at a time. Each predictor is divided into two groups and each group models the outcome variable for each group. This creates a "piecewise linear model" where each new feature is some proportion of the data. 

Group definitions are provided via linear regression models! Those with the smallest error are used. See [Kuhn and Johnson, 2016:145 ](http://appliedpredictivemodeling.com/) for more information. 

Fit the earth model
```{r}
# fit the model
earth1 <- earth(pressure ~ ., data=train, keepxy=TRUE, nprune=20, nfold=10) # pmethod?

# view summary output
summary(earth1)

# view predictor importance
evimp(earth1)

# compute predicted values
earth_pred <- predict(earth1)

# print accuracy
(mse <- mean((train$pressure - earth_pred)^2))
```

Earth plots
```{r}
# plot
plot(earth1)

# also
plotmo(earth1)

# 3d MARS plots!
# same syntactical rules apply here as well
plotmo(earth1, all2=TRUE, persp.col="azure")

plotmo(earth1, all2=TRUE, degree1=0, degree2=9,
       caption = "earth persp plot", 
       persp.col="seashell1") 
```

We can also see the ideal number of terms
```{r}
control <- trainControl(method = "repeatedcv",
                        repeats = 1, number = 1)

grid <- expand.grid(.degree = 1, .nprune = 2:25)

earth_best_terms <- train(pressure ~ ., data = train, method = "earth",
tuneGrid= grid)

earth_best_terms
plot(earth_best_terms)
```

ADD CV STUFF (nfold, ncross, trace)

ADD R-squared stuff

[See Stephen Milborrow's excellent notes on earth here](http://www.milbo.org/doc/earth-notes.pdf) for lots of handy tips and tricks.  

["earth" R package](https://cran.r-project.org/web/packages/earth/earth.pdf)

[Friedman 1991 - MARS](https://projecteuclid.org/download/pdf_1/euclid.aos/1176347963)

[Friedman 1993- Fast MARS](https://statistics.stanford.edu/sites/default/files/LCS%20110.pdf) 


