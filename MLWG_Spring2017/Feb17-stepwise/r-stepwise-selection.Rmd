---
title: ""
output: html_notebook
---

# Stepwise selection

Topics to cover:
* Best subset selection
* Forward stepwise selection
* Backward stepwise selection

**R packages**
```{r}
# Check if MASS is installed.
if (!require("MASS")) {
  # Install and load MASS package.
  install.packages("MASS")
  library(MASS)
}
```

## Background

Stepwise selection, or [stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression), is a commonly used technique to include a subset of covariates in a regression model. The goal is to increase accuracy compared to including all covariates in the model, because we can often improve model performance by removing some covariates (as we did with lasso / elastic net). Stepwise is a simple form of `feature selection` - choosing a subset of variables for incorporation into a machine learning algorithm.

Ideally we would test every possible combination of covariates and use the combination with the best performance. This is `best subset selection`.

Consider the case of three covariates: X1, X2, and X3. We would estimate the accuracy of the following models:

* All variables: X1, X2, X3 - our default regression
* X1 and X2 (exclude X3)
* X2 and X3 (exclude X1)
* X1 and X3 (exclude X2)
* X1-only
* X2-only
* X3-only
* No variables (intercept only)

The one with the best performance (e.g. cross-validated mean-squared error) is the one we would use. Stepwise algorithms are commonly used without cross-validation, and as a result they are usually overfitting the data - capturing random error in addition to true relationships in the data, resulting in worse performance on new data.

To generalize to any model size, if we have p covariates we would have to check $2^p$ different combinations: each covariate is either included or not (2 possibilities), so combining that for all covariates we have the product of p twos: $2 * 2 * 2...$ which simplifies to $2^p$. With 10 covariates that is 1,024 models to check, with 20 covariates it's a million, etc.

Stepwise selection is a simplification of best subset selection to make it computationally feasible for any number of covariates. It comes in three forms: forward, backward, and combined forward & backward.

`Forward selection` starts with just the intercept and considers which single variable to incorporate next. It loops over every variable, runs a regression with that variable plus the intercept, and chooses the variable with the best performance on a certain metric: $R^2$, [f-statistic](https://en.wikipedia.org/wiki/F-test), [Aikake Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion), or other preferred performance estimate. It then adds that variable to the model and considers the next variable to add, continuing to repeat until no remaining variable improves performance.

```{r}
?step
```

`Backward selection` does the same thing but it starts with all variables in the model and considers which variable to first remove from the model. It checks the performance for each variable when it is removed and removes the variable that is least useful to the regression performance. It continues this until no variable yields an increase in performance upon removal.


## Further reading

* Intro to Statistical Learning, section 6.1.2