---
title: "regression and splines" 
author: "Evan Muzzall"
date: "3/3/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1. Package installations
```{r}
if (FALSE) {
  install.packages("Zelig")
  devtools::install_github("ck37/ck37r")
}

library(splines) # call the base "splines"
library(Zelig) # this contains the "macro" dataset we will use 
```

##2. Goals
First, we will walk through linear regression, polynomial regression, polynomial splines, and smoothing splines using an incredibly simple example.  

Then, we will apply what we learned to do the same thing to the "macro" dataset from the "Zelig" package to see a more real life example.   

##3. Simple linear regression
Simple linear regression uses a single predictor/input/independent variable (X) to predict one target/outcome/response/dependent variably (Y). Ideally, we want to find the best estimates for B0 and B1 that minimize the error terms when using X to predict Y.  

```{r}
## generate toy predictors and responses
X <- c(2, 4, 8, 12, 18, 20)
Y <- c(1, 3, 5, 9, 19, 21)

## calculate means
mean(X)
mean(Y)

## calculate error for each observation
X-mean(X)
Y-mean(Y)

## plot the data
plot(x=X, y=Y, main="example")

## estimate B1 coefficient (slope)
B1 <- sum((X-mean(X)) * (Y-mean(Y))) / sum((X-mean(X))^2)
B1

## now estimate B0 coefficient (intercept)
B0 <- mean(Y) - (B1 * mean(X))
B0

## plot the abline
abline(B0, B1, col="black", lwd=2)
legend("topleft", inset=.0, c("linear"), lty=1, lwd=2, col="black", cex=.75)

## generate predicted values by plugging in our X values. 
Y_hat <- 0.4 + 0.8 * X
Y_hat

## calculate root mean sqaure error (RMSE) for our predictions:
Y_err <- Y_hat - Y
Y_err

## then, calculate the square of each of these errors and take the square root:
Y_err_sq <- Y_err^2
Y_err_sq

## the sum of Y_err_sq is the RMSE
RMSE <- sum(Y_err_sq)
RMSE

## divide by n and take square root
RMSE <- sqrt(sum(Y_err_sq) / length(Y))
RMSE

## sanity check
RMSE == sqrt(sum((Y_hat-Y)^2) / length(Y))

## double sanity check
lm_toy <- lm(Y ~ X)
lm_toy
summary(lm_toy)

## is our B1 the same as the slope generated by "lm" in R?
round(B1, digits=5) == round(lm_toy$coefficients[2], digits=5)

## is our B0 the same as the intercept generated by "lm" in R?
round(B0, digits=5) == round(lm_toy$coefficients[1], digits=5)
```

##4. Polynomial regression
However, it is not always advisable to assume linear relationships within data. Although linear models are flexible, they might not best express the relationships between your predictor and response variable. Thus, your resulting p-values might not accurately reflect the null hypothesis that the variables are not associated. 

Polynomial regression raises the original predictor variable to the n^th^ degree. These scalars act as a means to increase the fit of the model by assuming the point distributions are more parabolic shaped than linear.  

We will fit a 3^rd^ degree (cubic) polynomial so that our series of equations looks like this:  
Y ~ X  
Y ~ X^2  
Y ~ X^3  
```{r}
lm_poly1 <- lm(Y ~ X + I(X^2) + I(X^3))
lm_poly1
summary(lm_poly1)
## so, what is really happening here? 

## imagine we take our X variable and create a new column in a data frame that would look like this:
X2 <- X^2
X3 <- X^3

toy_df <- data.frame(Y, X, X2, X3)
toy_df

## the "poly" function produces the same results
lm_poly2 <- lm(Y ~ poly(X, 3, raw=FALSE))
lm_poly2
summary(lm_poly2)
#points(X ~ fitted(lm_poly2), col="blue")
lines(X ~ fitted(lm_poly2), lty=2, lwd=2, col="red")
legend("topleft", inset=.0, c("linear", "poly 3"), lty=c(1,2), lwd=2, col=c("black","red"), cex=.75)

## sanity check
lm_poly1$coefficients == lm_poly2$coefficients
```

##5. B-spline for polynomial splines
Polynomial splines are [piecewise polynomial functions](https://www.khanacademy.org/math/algebra/algebra-functions/piecewise-functions/v/piecewise-function-example) that form smoothed curved shapes at their junctions (called "knots"). The X predictor is divided into K regions, and a polynomial function is fit to the data within each region. This allows for greater flexibility than linear or polynomial fits. This is a k^th^ order spline where coefficients can be estimated by least squares.  

"B-spline" (basis-spline) is the function that allows for continuous joins at the spline knots. It is the marix that contains the information of the piecewise polynomial functions used to fit the spline.  
```{r}
## create xy data frame using our X and Y variables
xy <- data.frame(X, Y)
xy

bs(xy$X, df=3)
summary(ps1 <- lm(Y ~ bs(X, df=3), data = xy))

## example of safe prediction
summary(xy$X)
X_pred <- seq(1, 21, len = 200)
lines(X_pred, predict(ps1, data.frame(X=X_pred)), lty=3, lwd=2, col="green")
legend("topleft", inset=.0, c("linear", "poly 3", "b-spline"), lty=c(1,2,3), lwd=2, col=c("black","red", "green"), cex=.75)
```

##6. Smoothing splines
Smoothing splines are similar to the B-splines above, except they produce knots at each data point and coefficients of the estimated function are shrunk via regularization, thus helping prevent overfitting.  

Because each coefficient corresponds to a particular basis funciton, as sparsity increases the line gets straighter.  

Whereas the previous methods might be susceptible to overfitting, each data point is used as a knot and shrinking the coefficients. 
```{r}
smooth1 <- smooth.spline(y=Y, x=X, cv=FALSE, keep.data=TRUE, spar=NULL, penalty=1)
smooth1

lines(smooth1, col="blue", lty=4)
legend("topleft", inset=.0, c("linear", "poly 3", "b-spline", "smooth1"), lty=c(1,2,3,4), lwd=2, col=c("black","red", "green", "blue"), cex=.75)
```


##7. Repeat with the "macro" data from the "Zelig" package
```{r}
data(macro)
macro_lm <- lm(gdp ~ unem, data=macro)
macro_lm
summary(macro_lm)

## plot residuals
hist(macro_lm$residuals)

## plot it
plot(macro$unem, macro$gdp, col="gray80", 
     main="'macro' gdp ~ unem",
     xlab="unem deficit",
     ylab=)
summary(macro_lm)
macro_lm

abline(macro_lm$coefficients[1], macro_lm$coefficients[2],
       lwd=2, col="black")
legend("topleft", inset=.0, c("linear"), lty=1, lwd=2, col="black", cex=.75)

## generate predicted values by plugging in our X values. 
macro_pred <- predict(macro_lm, macro)

## or, this is the same as our formula way from the toy example:
macro_pred <- macro_lm$coefficients[1] + macro_lm$coefficients[2] * macro$unem

## check MSE on the predicted values
MSE <- mean(macro$gdp - macro_pred)^2
MSE
```

##9. Cubic polynomial regression
```{r}
poly_lm <- lm(gdp ~ poly(unem, 3, raw=FALSE), data=macro)
poly_lm
summary(poly_lm)

unem_lims <- range(macro$unem)
unem_grid <- seq(from=unem_lims[1], to=unem_lims[2])
poly_preds <- predict(poly_lm, newdata=list(unem=unem_grid), se=TRUE)

lines(unem_grid, poly_preds$fit, lty=2, lwd=2, col="red")
legend("topleft", inset=.0, c("linear", "ploy 4"), lty=c(1,2), lwd=2, col=c("black", "red"), cex=.75)


```

##10. B-spline
```{r}
bs(macro$unem, df=5)
summary(ps2 <- lm(gdp ~ bs(unem, df=5), data = macro))

## example of safe prediction
summary(macro$unem)
X_pred2 <- seq(0, 14, len = 200)
lines(X_pred2, predict(ps2, data.frame(unem=X_pred2)), lty=3, lwd=2, col="green")
legend("topleft", inset=.0, c("linear", "poly 3", "b-spline"), lty=c(1,2,3), lwd=2, col=c("black","red", "green"), cex=.75)

```

##11. Smoothing spline
```{r}
smooth2 <- smooth.spline(y=macro$gdp, x=macro$unem, cv=FALSE)
smooth2

lines(smooth2, col="blue", lty=4)
legend("topleft", inset=.0, c("linear", "poly 3", "b-spline", "smooth1"), lty=c(1,2,3,4), lwd=2, col=c("black","red", "green", "blue"), cex=.75)
```